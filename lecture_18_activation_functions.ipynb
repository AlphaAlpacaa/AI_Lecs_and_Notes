{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b7e20e",
   "metadata": {},
   "source": [
    "### Re-Producable yapmak için seed kullanmak lazım random_state\n",
    "- training_dataset_x, testing_dataset_x, training_dataset_y, testing_dataset_y = train_test_split(dataset_x, dataset_y, random_state=123)\n",
    "- model.fit(training_dataset_x, training_dataset_y, epochs=100,  batch_size=32, validation_split=0.2, shuffle=False)\n",
    "- model.add(Dense(64, activation=relu, kernel_initializer=GlorotUniform(seed=13579), name=\"Hidden-2\"))\n",
    "\n",
    "veya\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "gibi adımlarla eğitimdeki değerlerin korunması sağlanabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a2f7e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 192, 576, 192)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import relu \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/diabetes.csv')\n",
    "\n",
    "dataset_x = df.iloc[:, :-1].values\n",
    "dataset_y = df.iloc[:, -1].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_dataset_x, testing_dataset_x, training_dataset_y, testing_dataset_y = train_test_split(dataset_x, dataset_y, random_state=12345)\n",
    "len(training_dataset_x), len(testing_dataset_x), len(training_dataset_y), len(testing_dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "232adcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Sample\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Sample\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Hidden-1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden-2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Hidden-1 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden-2 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,689</span> (10.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,689\u001b[0m (10.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,689</span> (10.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,689\u001b[0m (10.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "model = Sequential(name=\"Sample\")\n",
    "\n",
    "model.add(Dense(64, activation=relu, kernel_initializer=GlorotUniform(seed=13579), input_shape=(dataset_x.shape[1],), name=\"Hidden-1\"))\n",
    "model.add(Dense(32, activation=relu, kernel_initializer=GlorotUniform(seed=13579), name=\"Hidden-2\"))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer=GlorotUniform(seed=13579), name=\"Output\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5679eb5",
   "metadata": {},
   "source": [
    "### Yapay Sinir Ağları Katmanları\n",
    "\n",
    "Saklı katmanlardaki nöron sayıları için çok pratik şeyler söylemek zordur. Çünkü saklı katmanlardaki nöron sayıları bazı faktörlere de bağlı olabilmektedir. Örneğin eğitimdeki kullanılacak veri miktarı, problemin karmaşıklığı, hyper parametrelerin durumları saklı katmanlardaki nöron sayısı üzerinde etkili olabilmektedir. Saklı katmanlardaki nöron sayıları için şunlar söylenebilir:\n",
    "\n",
    "Problem karmaşıklaştıkça saklı katmanlardaki nöron sayılarını artırmak uygun olabilmektedir.\n",
    "\n",
    "Saklı katmanlarda çok az nöron bulundurmak \"underfitting\" yani yetersiz öğrenmeye yol açabilmektedir.\n",
    "\n",
    "Saklı katmanlarda gereksiz biçimde fazla sayıda nöron bulundurmak hem eğitim süresini uzatabileceği gibi hem de \"overfitting\" durumuna yol açabilir.\n",
    "\n",
    "Eğitim veri kümesi azsa katmanlardaki nöron sayıları düşürülebilir.\n",
    "\n",
    "Pek çok problemde katmanlardaki nöron sayıları çok fazla ya da çok az olmadıktan sonra önemli olmayabilmektedir.\n",
    "\n",
    "Saklı katmanlardaki nöron sayısı girdi katmanındaki nöron sayısından az olmalıdır.\n",
    "\n",
    "Biz genellikle genel problemlerde iki saklı katman ve katmanlarda da 64, 100, 128 gibi sayılarda nöron kullanacağız. Ancak ileride özel mimarilerde bu durumu farklılaştıracağız.\n",
    "\n",
    "Yapay sinir ağı her farklı eğitimde farklı \"w\" ve \"bias\" değerlerini oluşturabilir. Yani ağın performansı eğitimden eğitime değişebilir. Her eğitimde ağın farklı değerlerle konumlandırılmasının nedenleri şunlardır:\n",
    "\n",
    "train_test_split fonksiyonu her çalıştırıldığında aslında fonksiyon training_dataset ve test_dataset veri kümelerini karıştırarak elde etmektedir.\n",
    "\n",
    "Katmanlardaki \"w\" değerleri programın her çalıştırılmasında rastgele bir başlangıç değeriyle set edilmektedir.\n",
    "\n",
    "fit işleminde her epoch sonrasında veri kümesi yeniden karıştırılmaktadır.\n",
    "\n",
    "Bir rastgele sayı üretiminde aynı üretim aynı \"tohum değerden (seed)\" başlatılırsa hep aynı değerler elde edilir. Bu duruma rassal sayı üretiminin \"reproducible\" olması denmektedir. Eğer tohum değer belirtilmezse bu tohum değer programın her çalıştırılmasında rastgele biçimde bilgisayarın saatinden alınmaktadır.\n",
    "\n",
    "O halde eğitimden hep aynı sonucun elde edilmesi için (yani eğitimin \"reproducible\" hale getirilmesi için) yukarıdaki unsurların dikkate alınması gerekir.\n",
    "\n",
    "Katmanlardaki aktivasyon fonksiyonları ne olmalıdır? Girdi katmanı gerçek bir katman olmadığında göre orada bir aktivasyon fonksiyonu yoktur.\n",
    "Saklı katmanlardaki aktivasyon fonksiyonları için çeşitli seçenekler bulunmaktadır. Ancak son yıllarda pek çok problemde en çok kullanılan ve başarısı görülmüş olan aktivasyon fonksiyonu \"relu (rectified linear unit)\" denilen fonksiyondur. Relu fonksiyonu şöyledir:\n",
    "\n",
    "x >= 0   ise y = x  \n",
    "x <  0   ise y = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "40de5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "rmsprop = RMSprop()\n",
    "model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24acef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - binary_accuracy: 0.4198 - loss: 7.3778 - val_binary_accuracy: 0.6121 - val_loss: 1.1081\n",
      "Epoch 2/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.5054 - loss: 1.2635 - val_binary_accuracy: 0.6207 - val_loss: 0.7250\n",
      "Epoch 3/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.5669 - loss: 0.9034 - val_binary_accuracy: 0.6466 - val_loss: 0.6899\n",
      "Epoch 4/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6024 - loss: 0.8804 - val_binary_accuracy: 0.5948 - val_loss: 0.7211\n",
      "Epoch 5/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6133 - loss: 0.8332 - val_binary_accuracy: 0.6379 - val_loss: 0.7044\n",
      "Epoch 6/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6302 - loss: 0.8427 - val_binary_accuracy: 0.6379 - val_loss: 0.6765\n",
      "Epoch 7/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6348 - loss: 0.8410 - val_binary_accuracy: 0.6207 - val_loss: 0.6795\n",
      "Epoch 8/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6308 - loss: 0.8564 - val_binary_accuracy: 0.6207 - val_loss: 0.6764\n",
      "Epoch 9/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6437 - loss: 0.8548 - val_binary_accuracy: 0.6466 - val_loss: 0.6824\n",
      "Epoch 10/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6368 - loss: 0.8430 - val_binary_accuracy: 0.6466 - val_loss: 0.6971\n",
      "Epoch 11/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6452 - loss: 0.8360 - val_binary_accuracy: 0.6379 - val_loss: 0.7042\n",
      "Epoch 12/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6429 - loss: 0.8271 - val_binary_accuracy: 0.6379 - val_loss: 0.7077\n",
      "Epoch 13/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6437 - loss: 0.8243 - val_binary_accuracy: 0.6552 - val_loss: 0.7348\n",
      "Epoch 14/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6464 - loss: 0.8255 - val_binary_accuracy: 0.6379 - val_loss: 0.7256\n",
      "Epoch 15/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6506 - loss: 0.8157 - val_binary_accuracy: 0.6379 - val_loss: 0.7202\n",
      "Epoch 16/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6569 - loss: 0.8013 - val_binary_accuracy: 0.6466 - val_loss: 0.7256\n",
      "Epoch 17/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6624 - loss: 0.8109 - val_binary_accuracy: 0.6552 - val_loss: 0.6935\n",
      "Epoch 18/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6615 - loss: 0.8090 - val_binary_accuracy: 0.6466 - val_loss: 0.7208\n",
      "Epoch 19/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6641 - loss: 0.7998 - val_binary_accuracy: 0.6638 - val_loss: 0.6949\n",
      "Epoch 20/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6540 - loss: 0.8064 - val_binary_accuracy: 0.6552 - val_loss: 0.7068\n",
      "Epoch 21/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6632 - loss: 0.7839 - val_binary_accuracy: 0.6810 - val_loss: 0.7224\n",
      "Epoch 22/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6601 - loss: 0.8043 - val_binary_accuracy: 0.6552 - val_loss: 0.7101\n",
      "Epoch 23/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6653 - loss: 0.7746 - val_binary_accuracy: 0.6638 - val_loss: 0.7375\n",
      "Epoch 24/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6614 - loss: 0.7924 - val_binary_accuracy: 0.6638 - val_loss: 0.7244\n",
      "Epoch 25/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6798 - loss: 0.7697 - val_binary_accuracy: 0.6638 - val_loss: 0.7234\n",
      "Epoch 26/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6602 - loss: 0.7924 - val_binary_accuracy: 0.6638 - val_loss: 0.7382\n",
      "Epoch 27/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6823 - loss: 0.7588 - val_binary_accuracy: 0.6466 - val_loss: 0.7372\n",
      "Epoch 28/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6715 - loss: 0.7841 - val_binary_accuracy: 0.6552 - val_loss: 0.7281\n",
      "Epoch 29/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6835 - loss: 0.7536 - val_binary_accuracy: 0.6466 - val_loss: 0.7300\n",
      "Epoch 30/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6706 - loss: 0.7732 - val_binary_accuracy: 0.6724 - val_loss: 0.7614\n",
      "Epoch 31/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6886 - loss: 0.7509 - val_binary_accuracy: 0.6466 - val_loss: 0.7314\n",
      "Epoch 32/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6807 - loss: 0.7587 - val_binary_accuracy: 0.6552 - val_loss: 0.7500\n",
      "Epoch 33/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7014 - loss: 0.7396 - val_binary_accuracy: 0.6638 - val_loss: 0.7557\n",
      "Epoch 34/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6852 - loss: 0.7663 - val_binary_accuracy: 0.6552 - val_loss: 0.7586\n",
      "Epoch 35/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6947 - loss: 0.7262 - val_binary_accuracy: 0.6466 - val_loss: 0.7360\n",
      "Epoch 36/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6897 - loss: 0.7300 - val_binary_accuracy: 0.6724 - val_loss: 0.7555\n",
      "Epoch 37/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6865 - loss: 0.7357 - val_binary_accuracy: 0.6293 - val_loss: 0.7559\n",
      "Epoch 38/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6996 - loss: 0.7161 - val_binary_accuracy: 0.6552 - val_loss: 0.7798\n",
      "Epoch 39/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6999 - loss: 0.7366 - val_binary_accuracy: 0.6466 - val_loss: 0.7450\n",
      "Epoch 40/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6883 - loss: 0.7276 - val_binary_accuracy: 0.6293 - val_loss: 0.7681\n",
      "Epoch 41/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6982 - loss: 0.7175 - val_binary_accuracy: 0.6466 - val_loss: 0.7688\n",
      "Epoch 42/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7031 - loss: 0.7103 - val_binary_accuracy: 0.6552 - val_loss: 0.7814\n",
      "Epoch 43/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6975 - loss: 0.7376 - val_binary_accuracy: 0.6293 - val_loss: 0.7512\n",
      "Epoch 44/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6970 - loss: 0.7049 - val_binary_accuracy: 0.6466 - val_loss: 0.7828\n",
      "Epoch 45/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7006 - loss: 0.7150 - val_binary_accuracy: 0.6724 - val_loss: 0.8001\n",
      "Epoch 46/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7015 - loss: 0.7246 - val_binary_accuracy: 0.6724 - val_loss: 0.7999\n",
      "Epoch 47/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6948 - loss: 0.7214 - val_binary_accuracy: 0.6379 - val_loss: 0.7759\n",
      "Epoch 48/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7066 - loss: 0.6837 - val_binary_accuracy: 0.6810 - val_loss: 0.7987\n",
      "Epoch 49/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6977 - loss: 0.7154 - val_binary_accuracy: 0.6638 - val_loss: 0.8053\n",
      "Epoch 50/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.6961 - loss: 0.7176 - val_binary_accuracy: 0.6293 - val_loss: 0.7791\n",
      "Epoch 51/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7081 - loss: 0.6791 - val_binary_accuracy: 0.6379 - val_loss: 0.7981\n",
      "Epoch 52/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7148 - loss: 0.6848 - val_binary_accuracy: 0.6466 - val_loss: 0.7941\n",
      "Epoch 53/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7097 - loss: 0.6807 - val_binary_accuracy: 0.6379 - val_loss: 0.7820\n",
      "Epoch 54/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7062 - loss: 0.6900 - val_binary_accuracy: 0.6552 - val_loss: 0.8164\n",
      "Epoch 55/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7076 - loss: 0.6715 - val_binary_accuracy: 0.6638 - val_loss: 0.8060\n",
      "Epoch 56/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7044 - loss: 0.6932 - val_binary_accuracy: 0.6552 - val_loss: 0.8000\n",
      "Epoch 57/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7272 - loss: 0.6658 - val_binary_accuracy: 0.6552 - val_loss: 0.8037\n",
      "Epoch 58/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7023 - loss: 0.6923 - val_binary_accuracy: 0.6638 - val_loss: 0.8176\n",
      "Epoch 59/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7107 - loss: 0.6792 - val_binary_accuracy: 0.6552 - val_loss: 0.8108\n",
      "Epoch 60/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7094 - loss: 0.6788 - val_binary_accuracy: 0.6466 - val_loss: 0.7916\n",
      "Epoch 61/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7120 - loss: 0.6745 - val_binary_accuracy: 0.6638 - val_loss: 0.8173\n",
      "Epoch 62/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7284 - loss: 0.6465 - val_binary_accuracy: 0.6638 - val_loss: 0.8213\n",
      "Epoch 63/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7022 - loss: 0.6889 - val_binary_accuracy: 0.6552 - val_loss: 0.8245\n",
      "Epoch 64/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7202 - loss: 0.6919 - val_binary_accuracy: 0.6466 - val_loss: 0.8348\n",
      "Epoch 65/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7389 - loss: 0.6404 - val_binary_accuracy: 0.6552 - val_loss: 0.8502\n",
      "Epoch 66/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7226 - loss: 0.6820 - val_binary_accuracy: 0.6638 - val_loss: 0.8268\n",
      "Epoch 67/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7183 - loss: 0.6436 - val_binary_accuracy: 0.6293 - val_loss: 0.8681\n",
      "Epoch 68/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7263 - loss: 0.6969 - val_binary_accuracy: 0.6466 - val_loss: 0.8365\n",
      "Epoch 69/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7340 - loss: 0.6373 - val_binary_accuracy: 0.6466 - val_loss: 0.8414\n",
      "Epoch 70/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7232 - loss: 0.6570 - val_binary_accuracy: 0.6466 - val_loss: 0.8391\n",
      "Epoch 71/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7373 - loss: 0.6504 - val_binary_accuracy: 0.6466 - val_loss: 0.8424\n",
      "Epoch 72/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7295 - loss: 0.6466 - val_binary_accuracy: 0.6466 - val_loss: 0.8397\n",
      "Epoch 73/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7374 - loss: 0.6317 - val_binary_accuracy: 0.6466 - val_loss: 0.8590\n",
      "Epoch 74/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7162 - loss: 0.6769 - val_binary_accuracy: 0.6293 - val_loss: 0.8610\n",
      "Epoch 75/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7363 - loss: 0.6483 - val_binary_accuracy: 0.6379 - val_loss: 0.8740\n",
      "Epoch 76/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7447 - loss: 0.6314 - val_binary_accuracy: 0.6379 - val_loss: 0.8468\n",
      "Epoch 77/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7306 - loss: 0.6352 - val_binary_accuracy: 0.6466 - val_loss: 0.9125\n",
      "Epoch 78/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7262 - loss: 0.6798 - val_binary_accuracy: 0.6293 - val_loss: 0.8622\n",
      "Epoch 79/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7437 - loss: 0.6124 - val_binary_accuracy: 0.6379 - val_loss: 0.8693\n",
      "Epoch 80/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7202 - loss: 0.6583 - val_binary_accuracy: 0.6379 - val_loss: 0.8492\n",
      "Epoch 81/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7288 - loss: 0.6316 - val_binary_accuracy: 0.6293 - val_loss: 0.8763\n",
      "Epoch 82/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7325 - loss: 0.6477 - val_binary_accuracy: 0.6293 - val_loss: 0.8920\n",
      "Epoch 83/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7372 - loss: 0.6194 - val_binary_accuracy: 0.6293 - val_loss: 0.8567\n",
      "Epoch 84/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7291 - loss: 0.6214 - val_binary_accuracy: 0.6466 - val_loss: 0.9067\n",
      "Epoch 85/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7330 - loss: 0.6556 - val_binary_accuracy: 0.6293 - val_loss: 0.8684\n",
      "Epoch 86/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7415 - loss: 0.6077 - val_binary_accuracy: 0.6466 - val_loss: 0.9067\n",
      "Epoch 87/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7220 - loss: 0.6599 - val_binary_accuracy: 0.6293 - val_loss: 0.8810\n",
      "Epoch 88/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7459 - loss: 0.5985 - val_binary_accuracy: 0.6293 - val_loss: 0.9037\n",
      "Epoch 89/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.7313 - loss: 0.6483 - val_binary_accuracy: 0.6293 - val_loss: 0.8899\n",
      "Epoch 90/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7563 - loss: 0.6016 - val_binary_accuracy: 0.6379 - val_loss: 0.9035\n",
      "Epoch 91/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7277 - loss: 0.6417 - val_binary_accuracy: 0.6293 - val_loss: 0.8864\n",
      "Epoch 92/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7484 - loss: 0.5996 - val_binary_accuracy: 0.6379 - val_loss: 0.9083\n",
      "Epoch 93/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7307 - loss: 0.6310 - val_binary_accuracy: 0.6293 - val_loss: 0.8968\n",
      "Epoch 94/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7484 - loss: 0.6075 - val_binary_accuracy: 0.6293 - val_loss: 0.8880\n",
      "Epoch 95/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7409 - loss: 0.6062 - val_binary_accuracy: 0.6293 - val_loss: 0.9116\n",
      "Epoch 96/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7302 - loss: 0.6327 - val_binary_accuracy: 0.6293 - val_loss: 0.9068\n",
      "Epoch 97/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7510 - loss: 0.5929 - val_binary_accuracy: 0.6293 - val_loss: 0.8985\n",
      "Epoch 98/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.7380 - loss: 0.6068 - val_binary_accuracy: 0.6293 - val_loss: 0.9141\n",
      "Epoch 99/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7321 - loss: 0.6161 - val_binary_accuracy: 0.6379 - val_loss: 0.9249\n",
      "Epoch 100/100\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.7369 - loss: 0.6081 - val_binary_accuracy: 0.6293 - val_loss: 0.9002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x228c80d6690>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_dataset_x, training_dataset_y, epochs=100,  batch_size=32, validation_split=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33b7a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.6990 - loss: 0.8502 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7892193794250488, 0.7135416865348816]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testing_dataset_x, testing_dataset_y, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "175c51ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'compile_metrics']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2160e",
   "metadata": {},
   "source": [
    "### Katmanlarda, Nöronlarda Kullanılan Aktivasyon Fonksiyonları \n",
    "\n",
    "Aktivasyon fonksiyonları isimsel girilebileceği gibi tensorflow.keras.activations modülündeki fonksiyonlar biçiminde de girilebilir.\n",
    "\n",
    "from tensorflow.keras.activations import relu\n",
    "...\n",
    "model.add(Dense(16, activation=relu, name='Hidden'))\n",
    "\n",
    "Bu modüldeki fonksiyonlar keras Tensorflow kullanılarak yazıldığı için çıktı olarak Tensor nesneleri vermektedir.\n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "İkili sınıflandırma problemlerinde çıktı katmanında en fazla kullanılan aktivasyon fonksiyonu \"sigmoid\" isimli fonksiyondur.\n",
    "Yukarıdaki \"diabet\" örneğinde biz çıktı katmanında sigmoid fonksiyonunu kullanmıştık. İkili sınıflandırma problemlerinde\n",
    "ağın çıktı katmanında tek bir nöron bulunur ve bu nöronun da aktivasyon fonksiyonu \"sigmoid\" fonksiyonudur.\n",
    "\n",
    "Peki sigmoid nasıl bir fonksiyondur? Bu fonksiyona \"lojistik (logistic)\" fonksiyonu da denilmektedir. Matematiksel ifadesi şöyledir:\n",
    "\n",
    "y = e ** x / (1 + e ** x)\n",
    "\n",
    "Burada e 2.71828... biçiminde irrasyonel e sayısıdır. Yukarıdaki kesrin pay ve paydası e ** x ile çarpılırsa fonksiyon aşağıdaki\n",
    "gibi de ifade edilebilir:\n",
    "\n",
    "y = 1 / (1 + e ** -x)\n",
    "Sigmoid eğrisi x = 0 için 0.5 değerini veren x pozitif yönde arttıkça 1 değerine yaklaşan, x negatif yönde arttıkça 0\n",
    "değerine yaklaşan S şeklinde bir eğridir. Sigmoid fonksiyonu (0, 1) arasında bir değer vermektedir. Bu nedenle çıktı katmanındaki\n",
    "nöronun aktivasyon fonksiyonu sigmoid ise ağdan sonuç olarak 0 ile 1 arasında noktalı bir sayı elde edilir. İşte biz de bu sayı\n",
    "0.5'ten büyükse onu 1 olarak, 0.5'ten küçükse 0 olarak değerlendiririz. Tabii aslında çıktı bir olasılık da belirtmektedir.\n",
    "Yani ağın çıktısı 1'e ne kadar yaklaşmışsa o kestirimin 1 olma olasılığı o kadar yüksek, ağın çıktısı 0'a ne kadar yaklaşmışsa\n",
    "o kestirimin 0 olma olasılığı o kadar yüksek olur.\n",
    "\n",
    "Sigmoid eğrisi aşağıdaki gibi çizilebilir.\n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = np.e ** x / (1 + np.e ** x)\n",
    "\n",
    "plt.title('Sigmoid (Logistic) Function', fontsize=14, pad=20, fontweight='bold')\n",
    "axis = plt.gca()\n",
    "axis.spines['left'].set_position('center')\n",
    "axis.spines['bottom'].set_position('center')\n",
    "axis.spines['top'].set_color(None)\n",
    "axis.spines['right'].set_color(None)\n",
    "\n",
    "axis.set_ylim(-1, 1)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n",
    "Aşağıda da sigmoid fonksiyonun birinci türevi alınmıştır. Sigmoid fonksiyonunun birinci türevi Gauss eğrisine benzemektedir.\n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "import sympy\n",
    "\n",
    "x = sympy.Symbol('x')\n",
    "fx = sympy.E ** x / (1 + sympy.E ** x)\n",
    "dx = sympy.diff(fx, x)\n",
    "\n",
    "from sympy import init_printing\n",
    "\n",
    "init_printing()\n",
    "\n",
    "print(dx)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.linspace(-10, 10, 1000)\n",
    "pdx = sympy.lambdify(x, dx)\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = pdx(x)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('First Derivative of Sigmoid Function', fontsize=14, pad=20, fontweight='bold')\n",
    "axis = plt.gca()\n",
    "axis.spines['left'].set_position('center')\n",
    "axis.spines['bottom'].set_position('center')\n",
    "axis.spines['top'].set_color(None)\n",
    "axis.spines['right'].set_color(None)\n",
    "\n",
    "axis.set_ylim(-1, 1)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c273bb",
   "metadata": {},
   "source": [
    "**Diger cok kullanılan bir aktivasyon fonksiyonu da \"hiperbolic tangent\" fonksiyonudur. Bu fonksiyon \"tanh\" ismiyle Keras'ta kullanılabilmektedir. Fonksiyonun matematiksel ifadesi şöyledir:**\n",
    "\n",
    "\\[ f(x) = (e^{**} (2 * x) - 1) / (e^{**} (2 * x) + 1) \\]\n",
    "\n",
    "**Tanh fonksiyonu adeta sigmoid fonksiyonunun (-1, 1) arası değer alan biçimi gibidir. Fonksiyon yine S şekli biçimindedir.**\n",
    "\n",
    "**Tanh fonksiyonu saklı katmanlarda da bazen çıktı katmanlarında da kullanılabilmektedir. Eskiden bu fonksiyon çok yoğun kullanılıyordu. Ancak artık saklı katmanlarda en çok ReLU fonksiyonu tercih edilmektedir.**  \n",
    "\n",
    "**Fonksiyonun grafiğini aşağıdaki gibi çizdirebiliriz:**\n",
    "\n",
    "```python\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "x = np.linspace(-10, 10, 1000)  \n",
    "y = (np.e^{**} (2 * x) - 1) / (np.e^{**} (2 * x) + 1)  \n",
    "\n",
    "plt.title('Hiperbolic Tangent (tanh) Function', fontsize=14, pad=20, fontweight='bold')  \n",
    "axis = plt.gca()  \n",
    "axis.spines['left'].set_position('center')  \n",
    "axis.spines['bottom'].set_position('center')  \n",
    "axis.spines['top'].set_color(None)  \n",
    "axis.spines['right'].set_color(None)  \n",
    "\n",
    "axis.set_ylim(-1, 1)  \n",
    "\n",
    "plt.plot(x, y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Diğer çok karşılaşılan bir aktivasyon fonksiyonu da \"softmax\" isimli fonksiyondur. Softmax fonksiyonu çok sınıflı sınıflandırma problemlerinde çıktı katmanlarında kullanılmaktadır. Örneğin bir resmin \"elma\", \"armut\", \"kayısı\", \"şeftali\", \"karpuz\" resimlerinden hangisi olduğunu anlamak için kullanılan sınıflandırma modeli çok sınıflı bir sınıflandırma modelidir. Buna istatistikte \"çok sınıflı lojistik regresyon (multinomial logistic regression)\" da denilmektedir. Bu tür problemlerde sinir ağında sınıf sayısı kadar nöron bulundurulur. Örneğin yukarıdaki resim sınıflandırma probleminde ağın çıktısında 5 nöron bulunmalıdır.**\n",
    "\n",
    "**Ağın çıktı katmanındaki tüm nöronların aktivasyon fonksiyonları softmax yapılırsa tüm çıktı katmanındaki nöronların çıktı değerlerinin toplamı her zaman 1 olur. Böylece adeta çıktı nöronlarının çıktı değerleri ilgili sınıfın olasılığını belirtmektedir. Biz de toplamı 1 olan çıktıların en yüksek değerine bakarız ve sınıflandırmanın o sınıfı kestirdiğini düşünürüz. Örneğin yukarıdaki resim sınıflandırmasında çıktı katmanında 5 nöron olacaktır. Bu nöronların çıktılarının toplamı da 1 olacaktır. Aşağıdaki gibi bir kestirim sonucu elde edilmiş olsun:**\n",
    "\n",
    "```\n",
    "Elma Nöronunun Çıktısı --> 0.2\n",
    "Armut Nöronunun Çıktısı --> 0.2\n",
    "Kayısı Nöronunun Çıktısı --> 0.3\n",
    "Şeftali Nöronunun Çıktısı --> 0.2\n",
    "Karpuz Nöronunun Çıktısı --> 0.2\n",
    "```\n",
    "\n",
    "**Burada en büyük çıktı 0.3 olan Kayısı nöronuna ilişkindir. O halde biz bu kestirimin \"kayısı\" olduğuna karar veririz. Softmax fonksiyonu şöyle hesaplanmaktadır. Elimizde aşağıdaki gibi x değerleri olsun:**\n",
    "\n",
    "```python\n",
    ">>> import numpy as np\n",
    ">>> x = np.array([3, 6, 4, 1, 7])\n",
    ">>> x\n",
    "array([3, 6, 4, 1, 7])\n",
    "```\n",
    "\n",
    "**Bu değerlerin softmax değerleri `np.e ** x / np.sum(e ** x)` biçimindedir.**\n",
    "\n",
    "```python\n",
    ">>> sm = np.e ** x / np.sum(np.e ** x)\n",
    ">>> sm\n",
    "array([0.0127328 , 0.25574518, 0.03461135, 0.0017232 , 0.69518747])\n",
    ">>> np.sum(sm)\n",
    "1.0\n",
    "```\n",
    "\n",
    "**Keras aslında çıktı katmanlarındaki tüm softmax aktivasyon fonksiyonlarının bir grup oluşturduğunu varsayar. Sonra girdilerle ağırlık değerlerini çarpıp bias değeriyle toplayarak yukarıdaki gibi bir x vektörü elde eder. Sonra da yukarıdaki işlemi uygular. Başka bir deyişle çıktı katmanındaki softmax aktivasyon fonksiyonuna sahip olan nöronlar bir grup olarak değerlendirilmektedir.**\n",
    "\n",
    "---\n",
    "\n",
    "**Diğer çok kullanılan aktivasyon fonksiyonlarından biri de \"linear\" aktivasyon fonksiyonudur. Aslında bu fonksiyon y = x fonksiyonudur. Yani girdi ile aynı değeri üretmektedir. Başka bir deyişle bir şey yapmayan bir fonksiyondur. Peki böyle bir aktivasyon fonksiyonunun ne anlamı olabilir? Bu aktivasyon fonksiyonu \"lojistik olmayan regresyon problemlerinde\" çıktı katmanında kullanılmaktadır.**  \n",
    "\n",
    "**Lojistik olmayan regresyon problemleri çıktı olarak bir sınıf bilgisi değil bir sayı bulmaya çalışan problemlerdir. Örneğin bir evin fiyatı, bir otomobilin mil başına yaktığı yakıt miktarı gibi problemler lojistik olmayan regresyon problemleridir. Bu aktivasyon fonksiyonu Keras'ta \"linear\" ismiyle kullanılmaktadır.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f0e9b",
   "metadata": {},
   "source": [
    "#### Relu Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "934ef876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7bc4a85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Relu Function\", fontsize=14, pad=20, fontweight='bold')\n",
    "\n",
    "axis = plt.gca()\n",
    "axis.spines['left'].set_position('center')\n",
    "axis.spines['bottom'].set_position('center')\n",
    "axis.spines['right'].set_color(None)\n",
    "axis.spines['top'].set_color(None)\n",
    "axis.set_ylim(-10, 10)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ba8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f61eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54bf7402",
   "metadata": {},
   "source": [
    "### Loss Fonksiyonları"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
